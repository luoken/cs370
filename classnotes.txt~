multi-processor vs multi-core:
cores share the same cache
best to use two next to each other rather than across board cores for coordination

cache : high speed ram, keep close if using constantly rather than putting in storage getting it back and forth.

usually 3 levels for cache l1, l2, l3. l3 usally further down/ slower 

wash rinse lather
how to partition, how to farm out, how to gather back

mapper - take what you have and distribute evenly in each box

reducer - take all the boxes and combine them so there will be one answer

look at threads. how to create threads in java
learn emac, vi, gcc


1/12 notes:

either break down instructions or break down data processor/ socket inside socket you have multiple cores stuff you use keep close. data locality
l1 -> l2
fast to slow

node: essentially a computer. 

computer -> sockets -> cores -> memory
memory on the core is called register

network for them to talk to each other
ethernet goes up to gigabit/ up to 10 gigabit
myrinet
infinity band

infiniband - one node connection from node to node to node

slow network - data collision/ not enough bandwidth
clusters are made to avoid the collisions.

private network : nonroutable network

flops : floating points operations

control unit -> instructions
arithmetic logic unit -> simple math

all modern architecture has pipelining
pipelining is sending instructions when its almost done like a pipeline

** know concept and terminology slide**

process - process is a program. consists of instructions and data
task - ex: print. a subset of a program. it is a collection of instructions

thread is where shared memory comes in. if you want shared memory you need multiple threads

symmetric multi-processor - might have equal right to the resources. if you have different processes you might want to favor one over the other.

synchronization - slows down to a certain extent parallel computation

granularity - can bundle as many instructions as you can then farm out

know your problem so you can find out if its course grain or fine grain. course is the bundling up

speedup should be bigger than 1

speed up is time it takes to execute / time it takes to execute parallels

portability - write one use in multiple places. achieved through software infrastructure you use. use standards. binary compatibility/ byte code/ standards
standards -> MPI
posix -> standard way of behavior of certain program. writing posix thread written in mac and should be able to be used in linux. binary wont work but code should function the same
should in theory work in minimum modifications. need to be recompiled

uma - everyone has the same space?

numa - alone but can be accessed by bus interconnect


***review everything up to parappel computer memory architectures. ***
 

<N> how wide
<z> how tall
<k> is what you want to find?
<m> how many threads



1.26
shared memories = threads. usually when mentioned its threads.
distrubuted memory is just like manager and worker. farming out work.
message passing is MPI
data parallel is called data streaming/GPU. so much thread that just farm out threads. fine grain computation
hybrid - a mixture of different techniques. 
single program multiple data (SPMD) 

process vs thread:
process - has to have 2 counter. one program counter and one data counter. only has one execution flow.
if you fork a program its a replicate program. 2 program counter. its a clone not a shared version
threading - having multiple counters. but data is the same. one data array but multiple threads.
replicate threading stack. 
critical region - you declare a piece of data. it is sacred. only one thread can touch it. mutex (mutually exclusive). if multiple thread touching the same thread its a problem

openMP has private data only for one thread 
not a lot of plumming just need to tag it and tell it.

Posix threads C language only
part of unix/linux operating systems
library based
commonly referred to as pthreads
very explicit parallelism; requores significant programmer attention to detail
gives you portability
need to do a lot of plumming

both are libraries but different models. 

MPI can send and receive. no platform restriction. linux can be sent to windows

communication - message passing. one node passing to another. data streaming when sending data to gpu
latency - the delay between data
bandwidth - the pipesize 

shooting data across the wire is better to send a large amount vs small packets at a time

synchronous - time. wait 
join is synchronous

asynchronous - just execute it.

semaphore - kinda like mutex. have to worry about when doing synchronication. 

data dependency won't let parallel work. 




message passing is pervasive
-easier to build than shared memory
 -pushes complexity off onto programmers
-world's biggest systems
 -RIKEN K Machine
-clusters
 -Beowulf
 -Dark systems
-laptops
Communicating: cooperative operations
-message-passing in an approach that akes the exchange of data cooperative
-data must both be explicitly sent and receive
-an advantage is that any change in the receiver's memory is made with the receiver's participation
Communicating: one-sided operations
-One-sided operations between parallel processes include remote memory reads and writes(gets and puts)
-Advantage: data can be accessed without waiting for another process
-Disadvantage: synchronization may be easy or difficult
Before MPI(circa 1990)
-Message passing well understood as parallel programming paradigm
 -As a model: Tony Hoare's Communicating Sequential Processes
 -Research demonstration: e.g., Caltech cosmic cube hybercube
-Early vendor systems were not portable
 -intel NX
 -thinking machine cmmd

mpi - message passing model
not a compiler
can't pass struct across the wire. have to decompose as array then recompose into struct
MPI used with huge cluster and lots and lots of data processes to do and you need point to point communication
you need to know who you're sending your data too
if no point to point commnication you use hadoop

mpicc hello -o hello.c
mpirun C -w hello

rank is id 
& is to get the value back


make file should be put in share folder
who is node who and who sends and who receives

worker has to run then compute and send back

manager doesnt do any work

manager should just be lazy

manager adds everything up?



